# Catastrophic vs existential risk
Epistemic status: hunch.

Imagine three global catastrophes:
1. 1% of humanity die (~78 million people)
2. 99% of humanity die (~6.93 billion people)
3. 100% of humanity die (~7.8 billion people)

Parfit ends Reasons and Persons by making the simple point that the difference in badness between (2) and (3) is far, far greater than the difference between (1) and (2), because (3) entails the non-existence of all future generations too.

As a matter of axiology, this claim seems hard to deny, unless:

a. you endorse a positive discount rate for welfare. 
b. you hold a person-affecting view which says that adding a happy person to the world does not increase its value. 

For now, let’s assume the axiological claim is true.

Axiology should inform our practical priorities—explicitly or not—but it does not determine them.

Rough definitions:
* Existential catastrophe: most of the potential value of the future is lost.
* Catastrophe: 1-99% of humanity die.

<!-- #todo What was Howie's preferred carving? Check Slack. -->

Right now, my impression is that the effective altruism community dedicates more resources to the mitigation of existential risk than to the mitigation of catastrophic risks. This may be correct, but I’ve not seen much discussion of what the resource allocation, in practice, should be. It seems at least possible that the community should spread resources between these two areas much more evenly, or even flip the allocation, such that more EA resources go to catastrophic than existential risks.

Reasons that might push in this direction:
* Catastrophes are very likely to occur this century (>80% likely?).
* Catastrophic risks may be much more tractable than existential risks (e.g. it’s easier to get people to care; mechanisms and feedback loops are somewhat easier to understand)
* Our ability to recover from catastrophes is very uncertain. What seem like “merely” catastrophic events may actually be existential. E.g. Global catastrophes may be among the largest “existential risk factors”.
* Moral uncertainty: proximity in time may actually count for something; simple expected value may be morally wrong; person-affecting views may have something to them.
* Simple expected value may be pragmatically suboptimal for managing risk, particularly for risk of ruin.
* You might think that better opportunities to work on existential risk lie in the future, and that we already have plenty of resources to spend on them (such that saving to spend on existential risk mitigation seems less attractive than spending on catastrophic risk mitigation).
* There might be much less of a trade off between the two issues than one might suppose.
* Non-altruistic reasons: we are the 99%!


## Todo
* Search EA Forum for relevant discussion
* Ask around

<!-- #web/fragments -->

<!-- {BearID:catastrophic-vs-existential-risk.md} -->
