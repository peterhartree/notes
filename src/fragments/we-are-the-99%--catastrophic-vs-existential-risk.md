# We are the 99%: catastrophic vs existential risk
Epistemic status: hunch; scant relevant investigation.

Imagine three global catastrophes:
1. 1% of humanity die
2. 99% of humanity die
3. 100% of humanity die

Parfit ends Reasons and Persons by pointing out that, perhaps counterintuitively, the difference in badness between (2) and (3) is far, far greater than the difference between (1) and (2), because (3) entails the non-existence of all future generations too.

As a matter of axiology, this seems hard to deny, unless you endorse a positive discount rate, or a person affecting view. So let’s just grant the point.

Axiology should inform our practical priorities (explicitly or not), but it should not determine them.

At the moment, the effective altruism community dedicates more resources to the mitigation of existential risk than to the mitigation of catastrophic risks. This may be right, but I think this question of practical priorities has been somewhat neglected. It seems at least possible that the community should spread resources between these two areas much more evenly, or even flip the allocation to the benefit of catastrophic risk mitigation.

Reasons that might push in this direction:
* Catastrophic risks are foreseeably highly probable this century.
* Catastrophic risks may be much more tractable than existential risks (e.g. it’s easier to get people to care; feedback loops somewhat clearer)
* Our ability to recover from catastrophes is highly uncertain: what look like “merely” catastrophic events may actually be existential. E.g. Global catastrophes may be among the largest “existential risk factors”
* Moral uncertainty type reasons: proximity may actually count for something; simple expected value may be morally wrong; person-affecting views may have something to them.
* Simple expected value may be pragmatically suboptimal
* You might think that existential risk is less urgent
* There might be much less of a trade off than one might suppose.
* Non-altruistic reasons: we are the 99%!


## Todo
* Search EA Forum for relevant discussion
* Ask around

<!-- #web/fragments -->

<!-- {BearID:we-are-the-99%--catastrophic-vs-existential-risk.md} -->
